## Estrategias para textos largos

- Procesa por tokens
- El texto se divide cuando se analiza
- Por ejemplo para gpt 4o el limite de tokens es: 8192 tokens
- GPT-3.5-turbo tiene un límite de tokens de 4096
- Bullets points es cuando se quiere una lista

Para medir cuantos tokens tiene un texto se puede usar la herramienta tokenizer en el link:

https://platform.openai.com/tokenizer

por ejemplo para:

```
    Golpe de Estado de Kornílov

    Kornílov y sus seguidores entre los altos oficiales, en su arresto en Býjov tras el fallido golpe de Estado.
    El golpe de Estado de Kornílov fue un intento fallido de golpe de Estado contrarrevolucionario llevado a cabo por el comandante en jefe del Ejército ruso, el general Lavr Kornílov, en septiembre de 1917. El Gobierno provisional ruso y los sóviets recibieron el apoyo mayoritario de la población e hicieron fracasar el golpe a los pocos días del alzamiento militar.

    Rusia participó desde el principio en la Primera Guerra Mundial, declarando la guerra a los Imperios Centrales el 2 de agosto de 1914. La Revolución de Febrero de 1917 acabó con la monarquía, pero no con la guerra. Convencido de que una victoria militar favorecería sus infructuosos intentos de alcanzar la paz reforzando la posición rusa ante sus aliados de la Triple Entente, el Gobierno organizó una ofensiva a comienzos del verano. Tras el fracaso de la Ofensiva de Kérenski en julio, el nuevo primer ministro, Aleksandr Kérenski, nombró al general Lavr Kornílov nuevo comandante en jefe del Ejército ruso; desde el comienzo surgieron roces entre ambos por los deseos del general de aplicar una serie de reformas conservadoras y opuestas a los consejos que, en su opinión, debían servir para mejorar la situación militar. Pronto Kornílov se convirtió en el candidato de las dispersas organizaciones conservadoras y de los representantes de la Entente para encabezar un nuevo Gobierno.

    A mediados de agosto, Kornílov presentó un plan de reforma que equivalía a la implantación de una dictadura militar y que Kérenski rechazó, a pesar del apoyo del viceministro de Defensa, el socialrevolucionario Sávinkov. Kornílov prefería aplicar sus medidas con la colaboración del Gobierno, pero se preparó para hacerlo incluso contra él; Kérenski, por su parte, deseaba tratar con él desde una postura reforzada por la Conferencia Estatal de Moscú, pero fracasó por la creciente polarización política.

    A finales de agosto y comienzos de septiembre parecía que Kérenski y Kornílov llegarían a un acuerdo para aplicar las reformas políticas y militares, pero la confusa intervención del antiguo procurador del Santísimo Sínodo Gobernante, Vladímir Lvov, confundió a ambos: Kornílov creyó que Kérenski había aceptado plenamente sus planes y este que Kornílov le presentaba un ultimátum y pretendía apartarlo del poder. El primer ministro forzó entonces la rebelión abierta del general al destituirlo; la marcha de este contra la capital resultó un fracaso, desbaratada por la resistencia no tanto del impotente Gobierno como de los sóviets y, especialmente, de los bolcheviques, que se recuperaron políticamente del revés sufrido en las Jornadas de Julio.
```

Tiene Tokens 768 Characters 2750

Estrategia:

1. Se determinan los tokens que consume el texto largo, dependiendo de la cantidad se divide en las cantidades necesarias dependiendo del consumo de los tokens, por ejemplo en 3 pedazos.
2. Abrir una nueva conversacion por cada pedazo, pidiendo un resumen del pedazo de texto con 30 palabras por ejemplo
3. Abrir otra nueva conversacion donde se peguen los 3 pedazos por ejemplo de respuesta en cada conversacion y volver a pedir el resumen.
4. Es necesario dividir el texto en partes más pequeñas al resumir un texto extenso en ChatGPT para facilitar la comprensión y asegurar un resumen más preciso y eficiente.


Dividir el texto en partes más pequeñas al resumir un texto largo en ChatGPT puede facilitar la comprensión del modelo y garantizar un resumen más preciso y eficiente. Esto se debe a que ChatGPT tiene un límite de caracteres para su procesamiento, y dividir el texto en partes más pequeñas ayuda a evitar posibles errores o la omisión de información importante.



## Playground

- Automatizar y usar las apis de chatgpt
- link: https://platform.openai.com/docs/overview
- La temperatura se usa para ajustar la creatividad del modelo, 0 menos creativo; y 2 es el mas creativo.
- Solo se puede usar con dinero disponible en la cuenta

![vista de playground](/imagenes/clase04/vista_playground.png)

### Conociendo el Playground

OpenAI Platform es una plataforma en línea desarrollada por OpenAI que permite a los usuarios experimentar y explorar los modelos de lenguaje de OpenAI de manera interactiva. Es un entorno donde puedes interactuar con el modelo de lenguaje, hacer preguntas y recibir respuestas generadas por el modelo en tiempo real.

Esa plataforma está dirigida a desarrolladores, científicos de datos, empresas y profesionales de IA que deseen aprovechar los recursos avanzados. Para acceder, es necesario iniciar sesión en la página de OpenAI platform.

La plataforma abarca varios productos y herramientas creados por OpenAI para ayudar en la creación e implementación de aplicaciones basadas en inteligencia artificial. Aquí hablaremos sobre uno de estos recursos disponibles: el Playground. Para acceder, simplemente haz clic en el botón 'Playground' en la parte superior de la página.


![pg_1](/imagenes/clase04/pg_1.png)


Después de hacer clic en Playground, accederemos a la página que se muestra en la figura siguiente. Con la opción “Mode” (Modo) siendo 'Chat', en el lado izquierdo hay un cuadro llamado “System” con el mensaje “You are a helpful assistent” (Eres un asistente útil) . En el centro de la pantalla se encuentra “User” con el mensaje “Enter a user message here” ('Ingresa un mensaje de usuario aquí) y un botón con signo positivo (+) para agregar un mensaje. En la parte inferior hay un botón “Submit” ( Enviar) en verde. Por último, en el lado derecho, hay varias opciones de configuración."

![pg_2](/imagenes/clase04/pg_2.png)

En la interfaz de OpenAI Playground, en el modo 'Chat' (charla), existe la opción de escribir un prompt en el campo "System" (Sistema). Esta es una forma de separar y especificar las tareas que deseas que el modelo de lenguaje realice. El campo "User"(Usuario) es donde puedes ingresar el texto a trabajar o incluso código, permitiéndote separar el contenido en caso de que tu instrucción y tu texto sean largos. Esta separación facilita la comprensión del modelo y ayuda a obtener resultados más precisos.

Es posible elegir qué modelo queremos utilizar en la configuración "Model" (modelo). Para quienes no son suscriptores de ChatGPT versión Plus, solo estará disponible el modelo 3.5. Además, hay algunas configuraciones disponibles en Playground que discutiremos en breve: Temperature (Temperatura), Maximum lenght (Longitud máxima), Top P, Frequency penalty (Penalización de frecuencia) y Presence penalty (Penalización de presencia).

Con estos recursos, Playground permite a los usuarios introducir indicaciones de texto, modificar parámetros y observar las respuestas del modelo, convirtiéndolo en un excelente entorno para trabajar, obtener respuestas optimizadas y generar automatizaciones.

    Es importante destacar que, en principio, Playground es gratuito, pero tiene un límite de tiempo de aproximadamente 3 a 4 meses. Al registrarte en una cuenta en OpenAI, recibes automáticamente un crédito inicial de $18, lo que permite generar alrededor de 650,000 palabras. Sin embargo, una vez que se alcance el límite de tiempo o se agoten los créditos, no podrás enviar mensajes a menos que pagues por utilizar el servicio.

### Ajustando las configuraciones

En Playground, puedes controlar los resultados de los prompts ajustando algunas configuraciones que no podemos controlar solo con ChatGPT. ¿Has experimentado el problema de obtener resultados diferentes cada vez que ejecutas un prompt? Existe una configuración que puede resolver este problema, y se llama **temperatura**.

Cuando configuramos la temperatura en 0 en Playground, el modelo siempre devolverá conclusiones idénticas o muy similares. Por otro lado, si aumentas la temperatura, los resultados no serán tan similares. Por ello, cuando la temperatura es superior a 0, enviar el mismo prompt produce conclusiones diferentes cada vez.

¿Y por qué sucede esto? El modelo predice que el texto es más probable que siga el texto anterior y la temperatura es un valor entre 0 y 2 que básicamente le permite controlar la confianza del modelo al hacer estas Predicciones. La reducción de temperatura significa que habrá menos riesgos y las conclusiones serán más precisas y deterministas. El aumento de la temperatura dará lugar a conclusiones más diversas.

Además de la temperatura, podemos modificar otros parámetros. El **'Maximum Length'** define la longitud máxima que una respuesta puede tener. Si estableces un límite numérico, el modelo detendrá la generación de texto una vez que alcance ese límite de caracteres. Esto es útil para controlar la extensión de las respuestas generadas y evitar resultados excesivamente largos.

En cuanto al **'Top P'**, también conocido como Muestreo de Núcleo, es una técnica que controla la diversidad de las respuestas considerando solo las probabilidades de las palabras más probables. Al establecer un valor para el 'Top P', como 1, por ejemplo, el modelo selecciona las palabras más probables hasta alcanzar una probabilidad acumulada del 100%. Esto evita que el modelo genere respuestas muy raras o improbables.

También contamos con la **'Penalización de Frecuencia'**, una configuración que controla la repetición de tokens en una respuesta. Establecer un valor más alto para la penalización de frecuencia anima al modelo a evitar repeticiones excesivas y producir respuestas más variadas.

Por último, el **Presence Penalty** (Penalización de Presencia) es la configuración que controla la preferencia del modelo por incluir o evitar información específica en las respuestas. Cuando aumentas el valor de la Penalización de Presencia, el modelo tiende más a evitar mencionar palabras o frases que hayas proporcionado como instrucción (prompt). Esto significa que el modelo intentará generar respuestas que no incluyan explícitamente la información proporcionada. Por otro lado, al disminuir el valor del Presence Penalty o establecerlo en cero, el modelo tendrá más libertad para incluir la información proporcionada en las respuestas. Esto puede resultar en respuestas más directas y que mencionen explícitamente lo que solicitaste.

La configuración de la Presence Penalty puede ser útil cuando deseas que el modelo genere respuestas más creativas, evitando depender excesivamente de la información proporcionada. Sin embargo, es importante observar que, en algunos casos, especialmente cuando las instrucciones son más largas o complejas, un valor alto de Penalización de Presencia puede hacer que el modelo ignore por completo la información proporcionada.

Estas configuraciones permiten personalizar el comportamiento del modelo de lenguaje según tus necesidades y ajustar el equilibrio entre creatividad, coherencia, longitud de las respuestas y otras características deseadas.

Es importante destacar que OpenAI Platform es un conjunto de servicios y herramientas en constante evolución, por lo que nuevos recursos y productos pueden añadirse a la plataforma con el tiempo. Por ello, es recomendable consultar el sitio web de OpenAI o la documentación oficial para obtener información actualizada sobre los recursos disponibles en OpenAI Platform. 

